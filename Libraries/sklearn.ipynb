{"cells":[{"cell_type":"code","execution_count":32,"metadata":{},"outputs":[],"source":["# #####################\n","# # sklearn\n","# #####################\n","\n","# 'PreProcessing'\n","from sklearn.model_selection import train_test_split, cross_val_score, cross_val_predict, GridSearchCV, KFold, StratifiedKFold, GridSearchCV\n","from sklearn import preprocessing\n","from sklearn.preprocessing import PolynomialFeatures\n","\n","# 'Models'\n","from sklearn.linear_model import LinearRegression\n","from sklearn.linear_model import Ridge\n","from sklearn.neighbors import KNeighborsClassifier\n","from sklearn.linear_model import LogisticRegression, LogisticRegressionCV\n","from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor, plot_tree\n","from sklearn.ensemble import BaggingClassifier, RandomForestClassifier, AdaBoostClassifier\n","from sklearn.metrics import mean_squared_error\n","from sklearn.metrics import accuracy_score \n","from sklearn.preprocessing import StandardScaler\n","\n","\n","\n","\n","\n","# 'Others'\n","from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n","from sklearn.cluster import KMeans\n","from sklearn.metrics import roc_curve, auc, confusion_matrix, ConfusionMatrixDisplay"]},{"cell_type":"markdown","metadata":{},"source":["# Scaling and Normalizing Features "]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import numpy as np\n","# Example dataset\n","np.random.seed(0) # By using np.random.seed(0), the random numbers generated remain consistent, making your experiments predictable and easier to analyze.\n","X_train = np.random.rand(5, 3) * 10  # Training data\n","X_test = np.random.rand(3, 3) * 10   # Test data\n","\n","\n","# 1. Using preprocessing.scale (function)\n","# Standardize columns to 0 mean and 1 standard deviation\n","# The operation ensures that the features in your dataset have a standard normal distribution (mean = 0, standard deviation = 1).\n","X_scaled = preprocessing.scale(X_train)  # Standardized training data\n","print(\"Scaled Training Data:\\n\", X_scaled)\n","\n","\n","# 2. Using StandardScaler (class with Transformer API)\n","# Standardize features by removing the mean and scaling to unit variance\n","# It learns the mean and standard deviation from the training data (fit) and applies the same transformation to both the training and testing data (transform).\n","standardizer = preprocessing.StandardScaler()\n","\n","# Fit the scaler on training data and transform it\n","X_train_standardized = standardizer.fit_transform(X_train)\n","X_test_standardized = standardizer.transform(X_test)\n","\n","print(\"\\nStandardized Training Data:\\n\", X_train_standardized)\n","print(\"\\nStandardized Test Data:\\n\", X_test_standardized)\n","\n","# 3. Using Normalizer (class with Transformer API)\n","# Normalize data to unit norm (l2 norm by default)\n","# normalizer = preprocessing.Normalizer().fit(X_train.T)  # Fit normalizer on transposed training data\n","normalizer = preprocessing.Normalizer()  # Fit normalizer \n","\n","# Transform training and test data\n","X_train_normalized = normalizer.transform(X_train.T).T  # Normalize training data\n","X_test_normalized = normalizer.transform(X_test.T).T    # Normalize test data\n","\n","\n","\n","print(\"\\nNormalized Training Data:\\n\", X_train_normalized)\n","print(\"\\nNormalized Test Data:\\n\", X_test_normalized)\n"]},{"cell_type":"code","execution_count":13,"id":"b524765b-66be-4ff6-b697-f0a13fc3b6b7","metadata":{},"outputs":[],"source":["import pandas as pd\n","\n","# Import clean data \n","path = 'https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-DA0101EN-SkillsNetwork/labs/Data%20files/module_5_auto.csv'\n","df = pd.read_csv(path)"]},{"cell_type":"code","execution_count":14,"id":"3df6f370-2ef8-41d2-a1eb-ffc0d828b5e4","metadata":{},"outputs":[],"source":["df.to_csv('module_5_auto.csv')"]},{"cell_type":"markdown","id":"e555421f-03a0-49ab-aff5-93500bb740b1","metadata":{},"source":[" First, let's only use numeric data:\n"]},{"cell_type":"code","execution_count":null,"id":"d2b40bd0-2061-4206-a189-29d01f8162e6","metadata":{},"outputs":[],"source":["df=df._get_numeric_data()\n","df.head()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["df.columns"]},{"cell_type":"markdown","metadata":{},"source":["<h2 id=\"ref1\"> Training and Testing</h2>\n"]},{"cell_type":"markdown","metadata":{},"source":["Is a method used to split the data into training and test sets. You use the training set to train a model, discover possible predictive relationships, and then use the test set to test your model to evaluate its performance."]},{"cell_type":"code","execution_count":null,"id":"d2fe224a-4162-4fb4-99f2-314eaa6646bf","metadata":{},"outputs":[],"source":["# An important step in testing your model is to split your data into training and testing data. We will place the target data price in a separate dataframe y_data\n","df = df.dropna()\n","\n","y_data = df['price']\n","\n","# Drop price data in dataframe x_data:\n","x_data=df.drop('price',axis=1)\n","\n","\n","# Now, we randomly split our data into training and testing data using the function train_test_split\n","x_train, x_test, y_train, y_test = train_test_split(x_data, y_data, test_size=0.10, random_state=1)\n","\n","\n","print(\"number of test samples :\", x_test.shape[0])\n","print(\"number of training samples:\",x_train.shape[0])\n","\n","\n","#  We create a Linear Regression object:\n","lre=LinearRegression()\n","\n","# We fit the model using the feature \"horsepower\"\n","lre.fit(x_train[['horsepower']], y_train)\n","\n","# Let's calculate the R^2 on the test and training data:\n","lre.score(x_test[['horsepower']], y_test)\n","lre.score(x_train[['horsepower']], y_train)"]},{"cell_type":"markdown","id":"c6be9e6e-a690-4b11-b9fc-2d104816bbb9","metadata":{},"source":["Sometimes you do not have sufficient testing data; as a result, you may want to perform cross-validation. Let's go over several methods that you can use for cross-validation. \n"]},{"cell_type":"markdown","metadata":{},"source":["## Cross-Validation\n","## We now have two approaches for cross-validation but they differ in implementation, workflow, and output.\n","\n"]},{"cell_type":"markdown","metadata":{},"source":["\n","| Feature                          | `KFold` (Manual)                         | `StratifiedKFold`                        | `cross_val_score` / `cross_val_predict` | `KNN Regression`                       |\n","|----------------------------------|------------------------------------------|------------------------------------------|-----------------------------------------|----------------------------------------|\n","| **Control**                      | Full control over splitting, training, and evaluation. | Preserves class distribution across folds; control over splitting. | Limited control—optimized for simplicity. | Predictions rely on \\( k \\)-nearest neighbors; less overall control. |\n","| **Ease of Use**                  | Requires more manual steps and custom logic. | Requires some manual handling like `KFold`. | Single function call for scoring or prediction. | Requires scaling and careful parameter tuning. |\n","| **Output**                       | Metrics (like MSE) need to be computed manually for each fold. | Metrics computed manually; suitable for classification. | Directly returns scores (e.g., R², MSE) or predictions. | Predicts using the average of neighbors’ target values. |\n","| **Flexibility**                  | Can include custom logic, transformations, or metrics. | Suitable for imbalanced classification problems. | Limited to the metrics and options provided by `scikit-learn`. | Supports multiple distance metrics; adaptable to non-linear relationships. |\n","| **Performance Prediction**       | Requires separate prediction logic.      | Requires separate prediction logic.      | `cross_val_predict` generates predictions for the entire dataset. | Provides instance-specific predictions based on nearest neighbors. |\n","| **Typical Use Cases**            | Advanced or custom workflows.            | Classification with imbalanced datasets. | Standard cross-validation workflows.     | Localized predictions for regression tasks, especially with non-linear data. |\n","\n"]},{"cell_type":"markdown","metadata":{},"source":["<h2>KFold for Cross-Validation</h2>\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Initialize KFold\n","K = 5  # Number of folds\n","kf = KFold(n_splits=K, shuffle=True, random_state=42)\n","\n","# Initialize model\n","model = LinearRegression()\n","\n","# Store metrics for each fold\n","mse_scores = []\n","\n","# K-Fold Cross-Validation\n","for i, (train_index, val_index) in enumerate(kf.split(x_train)):\n","    print(f\"Fold {i+1}\")\n","    \n","    # Create train and validation sets for this fold\n","    x_fold_train, x_fold_val = x_train.iloc[train_index], x_train.iloc[val_index]\n","    y_fold_train, y_fold_val = y_train.iloc[train_index], y_train.iloc[val_index]\n","    \n","    # Train the model on the training set\n","    model.fit(x_fold_train, y_fold_train)\n","    \n","    # Predict on the validation set\n","    y_val_pred = model.predict(x_fold_val)\n","    \n","    # Calculate MSE for this fold\n","    mse = mean_squared_error(y_fold_val, y_val_pred)\n","    mse_scores.append(mse)\n","    \n","    print(f\"MSE for Fold {i+1}: {mse:.4f}\\n\")\n","\n","# Report average MSE across all folds\n","average_mse = np.mean(mse_scores)\n","print(f\"Average Mean Squared Error across {K} folds: {average_mse:.4f}\")\n","\n","# Evaluate on the hold-out test set\n","y_test_pred = model.predict(x_test)\n","test_mse = mean_squared_error(y_test, y_test_pred)\n","print(f\"Mean Squared Error on Hold-Out Test Set: {test_mse:.4f}\")"]},{"cell_type":"markdown","metadata":{},"source":["<h2>StratifiedKFold</h2>"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["df['price_category'] = pd.qcut(df['price'], q=2, labels=[0, 1])  # Convert 'price' to binary classes: To make it suitable for classification, it's split into two categories (high/low price) \n","\n","# Extract features and target\n","x_data = df.drop(['price', 'price_category'], axis=1)\n","y_data = df['price_category']\n","\n","# Binarize categorical features if any (not necessary for purely numerical data)\n","x_data = pd.get_dummies(x_data, drop_first=True)\n","\n","# Initialize StratifiedKFold\n","K = 5\n","skf = StratifiedKFold(n_splits=K, shuffle=True, random_state=42)\n","\n","# Initialize model\n","model = LogisticRegression(max_iter=1000)\n","\n","# Store accuracy for each fold\n","accuracy_scores = []\n","\n","# Stratified K-Fold Cross-Validation\n","for i, (train_index, test_index) in enumerate(skf.split(x_data, y_data)):\n","    print(f\"Fold {i+1}\")\n","    \n","    # Create train and test sets for this fold\n","    X_train, X_test = x_data.iloc[train_index], x_data.iloc[test_index]\n","    Y_train, Y_test = y_data.iloc[train_index], y_data.iloc[test_index]\n","    \n","    # Train the model on the training set\n","    model.fit(X_train, Y_train)\n","    \n","    # Predict on the test set\n","    Y_pred = model.predict(X_test)\n","    \n","    # Calculate accuracy for this fold\n","    accuracy = accuracy_score(Y_test, Y_pred)\n","    accuracy_scores.append(accuracy)\n","    \n","    print(f\"Accuracy for Fold {i+1}: {accuracy:.4f}\\n\")\n","\n","# Report average accuracy across all folds\n","average_accuracy = sum(accuracy_scores) / K\n","print(f\"Average Accuracy across {K} folds: {average_accuracy:.4f}\")"]},{"cell_type":"markdown","id":"46d238f4-4d10-4819-897d-3ec9c913bf84","metadata":{},"source":["<h2>Cross-Validation Score</h2>\n"," cross-validation works by splitting the data into folds where you use some of the folds as a training set, which we use to train the model, and the remaining parts are used as a test set, which we use to test the model. You iterate through the folds until you use each partition for training and testing. At the end, you average results as the estimate of out-of-sample error."]},{"cell_type":"code","execution_count":null,"id":"0ce2ff73-a3c3-44ea-befe-7ea149d19361","metadata":{},"outputs":[],"source":["# We input the object, the feature (\"horsepower\"), and the target data (y_data). The parameter 'cv' determines the number of folds. In this case, it is 4. \n","Rcross = cross_val_score(lre, x_data[['horsepower']], y_data, cv=4)\n","\n","# The default scoring is R^2. Each element in the array has the average R^2 value for the fold:\n","Rcross\n","#  We can calculate the average and standard deviation of our estimate:\n","print(\"The mean of the folds are\", Rcross.mean(), \"and the standard deviation is\" , Rcross.std())\n","\n","# We can use negative squared error as a score by setting the parameter  'scoring' metric to 'neg_mean_squared_error'. \n","-1 * cross_val_score(lre,x_data[['horsepower']], y_data,cv=4,scoring='neg_mean_squared_error')\n","\n","# now use 'cross_val_predict' to predict the output. The function splits up the data into the specified number of folds, with one fold for testing and the other folds are used for training. First, import the function:\n","yhat = cross_val_predict(lre,x_data[['horsepower']], y_data,cv=4)\n","yhat[0:5]"]},{"cell_type":"markdown","metadata":{},"source":["<h2>K-Nearest Neighbors (KNN)</h2>\n","KNN classifier is a versatile and straightforward machine learning algorithm used for both classification and regression tasks. The core idea of KNN revolves around identifying the closest data points in the feature space to make predictions."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["X = df.drop(columns=['price', 'price_category'], errors='ignore')  # Features\n","y = df['price_category']  # Target (classification)\n","\n","# Split data into training and testing sets\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","\n","# =====================\n","# Initialize KNN Classifier\n","# =====================\n","k = 5  # Number of neighbors\n","knn = KNeighborsClassifier(n_neighbors=k, weights='uniform', metric='euclidean')\n","\n","# Train the model\n","knn.fit(X_train, y_train)\n","\n","# Make predictions on the test set\n","y_pred = knn.predict(X_test)\n","\n","# =====================\n","# Evaluate the Model\n","# =====================\n","# Accuracy Score\n","accuracy = accuracy_score(y_test, y_pred)\n","print(f\"Test Accuracy: {accuracy:.4f}\")\n","\n","# Confusion Matrix\n","cm = confusion_matrix(y_test, y_pred)\n","print(\"\\nConfusion Matrix:\")\n","print(cm)\n","\n","# =====================\n","# Cross-Validation for Performance Estimation\n","# =====================\n","cv_scores = cross_val_score(knn, X, y, cv=5, scoring='accuracy')\n","print(f\"\\nCross-Validation Accuracy: {cv_scores.mean():.4f} ± {cv_scores.std():.4f}\")\n","\n"]},{"cell_type":"markdown","id":"2850196d-072a-4484-bcc4-bdaae8b7cfee","metadata":{},"source":["<h2 id=\"ref2\"> Overfitting, Underfitting and Model Selection</h2>\n","\n","<p>It turns out that the test data, sometimes referred to as the \"out of sample data\", is a much better measure of how well your model performs in the real world.  One reason for this is overfitting.\n","\n","Let's go over some examples. It turns out these differences are more apparent in Multiple Linear Regression and Polynomial Regression so we will explore overfitting in that context.</p>\n"]},{"cell_type":"code","execution_count":null,"id":"eae0737e-61fa-47c5-90f0-c8165ebad949","metadata":{},"outputs":[],"source":["# Let's create Multiple Linear Regression objects and train the model using 'horsepower', 'curb-weight', 'engine-size' and 'highway-mpg' as features.\n","\n","lr = LinearRegression()\n","lr.fit(x_train[['horsepower', 'curb-weight', 'engine-size', 'highway-mpg']], y_train)\n","\n","# Prediction using training data:\n","yhat_train = lr.predict(x_train[['horsepower', 'curb-weight', 'engine-size', 'highway-mpg']])\n","yhat_train[0:5]\n","\n","# Prediction using test data: \n","yhat_test = lr.predict(x_test[['horsepower', 'curb-weight', 'engine-size', 'highway-mpg']])\n","yhat_test[0:5]"]},{"cell_type":"markdown","metadata":{},"source":["Let's perform some model evaluation using our training and testing data separately. \n"]},{"cell_type":"code","execution_count":33,"metadata":{},"outputs":[],"source":["import matplotlib.pyplot as plt\n","%matplotlib inline\n","import seaborn as sns\n","\n","def DistributionPlot(RedFunction, BlueFunction, RedName, BlueName, Title):\n","    width = 12\n","    height = 10\n","    plt.figure(figsize=(width, height))\n","\n","    ax1 = sns.kdeplot(RedFunction, color=\"r\", label=RedName)\n","    ax2 = sns.kdeplot(BlueFunction, color=\"b\", label=BlueName, ax=ax1)\n","\n","    plt.title(Title)\n","    plt.xlabel('Price (in dollars)')\n","    plt.ylabel('Proportion of Cars')\n","    plt.show()\n","    plt.close()\n","\n","\n","Title = 'Distribution  Plot of  Predicted Value Using Training Data vs Training Data Distribution'\n","DistributionPlot(y_train, yhat_train, \"Actual Values (Train)\", \"Predicted Values (Train)\", Title)"]},{"cell_type":"markdown","id":"51a98d37-9f56-4178-82f1-1a84238ad8f5","metadata":{},"source":["The fig predicted values using the training data compared to the actual values of the training data. Where, the model seems to be doing well in learning from the training dataset. But what happens when the model encounters new data from the testing dataset? When the model generates new values from the test data, we see the distribution of the predicted values is much different from the actual target values. \n"]},{"cell_type":"code","execution_count":null,"id":"51882274-4079-48c3-b4a6-3a4b0c28397d","metadata":{},"outputs":[],"source":["Title='Distribution  Plot of  Predicted Value Using Test Data vs Data Distribution of Test Data'\n","DistributionPlot(y_test,yhat_test,\"Actual Values (Test)\",\"Predicted Values (Test)\",Title)"]},{"cell_type":"markdown","id":"e1a257cc-36bd-4665-bcfc-24028b694792","metadata":{},"source":["<p>Comparing Figure 1 and Figure 2, it is evident that the distribution of the test data in Figure 1 is much better at fitting the data. This difference in Figure 2 is apparent in the range of 5000 to 15,000. This is where the shape of the distribution is extremely different. Let's see if polynomial regression also exhibits a drop in the prediction accuracy when analysing the test dataset.</p>\n"]},{"cell_type":"markdown","id":"90f57b56-d8d5-4f3c-99b5-6e7c9a84cdf0","metadata":{},"source":["## Overfitting\n","\n","Overfitting occurs when the model fits the noise, but not the underlying process. Therefore, when testing your model using the test set, your model does not perform as well since it is modelling noise, not the underlying process that generated the relationship. Let's create a degree 5 polynomial model.\n"]},{"cell_type":"code","execution_count":null,"id":"4feca416-a414-4d6d-b428-e9bc1cb06a79","metadata":{},"outputs":[],"source":["# Let's use 55 percent of the data for training and the rest for testing:\n","x_train, x_test, y_train, y_test = train_test_split(x_data, y_data, test_size=0.45, random_state=0)\n","\n","# We will perform a degree 5 polynomial transformation on the feature 'horsepower'\n","pr = PolynomialFeatures(degree=5)\n","x_train_pr = pr.fit_transform(x_train[['horsepower']])\n","x_test_pr = pr.fit_transform(x_test[['horsepower']])\n","pr\n","\n","# Now, let's create a Linear Regression model \"poly\" and train it.\n","poly = LinearRegression()\n","poly.fit(x_train_pr, y_train)\n","\n","# We can see the output of our model using the method \"predict.\" We assign the values to \"yhat\".\n","yhat = poly.predict(x_test_pr)\n","yhat[0:5]\n","\n","# Let's take the first five predicted values and compare it to the actual targets. \n","print(\"Predicted values:\", yhat[0:4])\n","print(\"True values:\", y_test[0:4].values)"]},{"cell_type":"markdown","id":"9a7afdbb-26f8-4901-9b48-dcf00ac03e3c","metadata":{},"source":["We will use the function \"PollyPlot\" that we defined at the beginning of the lab to display the training data, testing data, and the predicted function.\n"]},{"cell_type":"code","execution_count":42,"metadata":{},"outputs":[],"source":["def PollyPlot(xtrain, xtest, y_train, y_test, lr,poly_transform):\n","    width = 12\n","    height = 10\n","    plt.figure(figsize=(width, height))\n","    \n","    \n","    #training data \n","    #testing data \n","    # lr:  linear regression object \n","    #poly_transform:  polynomial transformation object \n"," \n","    xmax=max([xtrain.values.max(), xtest.values.max()])\n","\n","    xmin=min([xtrain.values.min(), xtest.values.min()])\n","\n","    x=np.arange(xmin, xmax, 0.1)\n","\n","\n","    plt.plot(xtrain, y_train, 'ro', label='Training Data')\n","    plt.plot(xtest, y_test, 'go', label='Test Data')\n","    plt.plot(x, lr.predict(poly_transform.fit_transform(x.reshape(-1, 1))), label='Predicted Function')\n","    plt.ylim([-10000, 60000])\n","    plt.ylabel('Price')\n","    plt.legend()"]},{"cell_type":"code","execution_count":null,"id":"d308be26-8ccf-4187-b42c-ee3b2b64f0ca","metadata":{},"outputs":[],"source":["PollyPlot(x_train['horsepower'], x_test['horsepower'], y_train, y_test, poly,pr)"]},{"cell_type":"markdown","id":"64b123e4-ed05-475d-97e2-14a482c39735","metadata":{},"source":["Figure 3: A polynomial regression model where red dots represent training data, green dots represent test data, and the blue line represents the model prediction. \n"]},{"cell_type":"markdown","id":"047d5e98-ae4b-4b57-a800-426be5e9b6ff","metadata":{},"source":["We see that the estimated function appears to track the data but around 200 horsepower, the function begins to diverge from the data points. \n"]},{"cell_type":"code","execution_count":null,"id":"92add389-fa5d-4003-88c3-2ed42d6a8e44","metadata":{},"outputs":[],"source":["#  R^2 of the training data:\n","poly.score(x_train_pr, y_train)\n","\n","\n","#  R^2 of the test data:\n","poly.score(x_test_pr, y_test)"]},{"cell_type":"markdown","id":"16a86178-4446-4bdf-946d-6723c699ec98","metadata":{},"source":["We see the R^2 for the training data is 0.5567 while the R^2 on the test data was -29.87.  The lower the R^2, the worse the model. A negative R^2 is a sign of overfitting.\n"]},{"cell_type":"markdown","id":"3195257e-ab8e-44f6-9cf9-0016474aec53","metadata":{},"source":["Let's see how the R^2 changes on the test data for different order polynomials and then plot the results:\n"]},{"cell_type":"code","execution_count":null,"id":"89850c44-19a3-4a15-a870-cc666072a7db","metadata":{},"outputs":[],"source":["Rsqu_test = []\n","\n","order = [1, 2, 3, 4]\n","for n in order:\n","    pr = PolynomialFeatures(degree=n)\n","    \n","    x_train_pr = pr.fit_transform(x_train[['horsepower']])\n","    \n","    x_test_pr = pr.fit_transform(x_test[['horsepower']])    \n","    \n","    lr.fit(x_train_pr, y_train)\n","    \n","    Rsqu_test.append(lr.score(x_test_pr, y_test))\n","\n","plt.plot(order, Rsqu_test)\n","plt.xlabel('order')\n","plt.ylabel('R^2')\n","plt.title('R^2 Using Test Data')\n","plt.text(3, 0.75, 'Maximum R^2 ')    \n","\n"]},{"cell_type":"markdown","id":"76422a9a-5203-4328-a43f-e50e158f6943","metadata":{},"source":["We see the R^2 gradually increases until an order three polynomial is used. Then, the R^2 dramatically decreases at an order four polynomial.\n"]},{"cell_type":"markdown","metadata":{},"source":["## Ridge Regression\n","You should use ridge regression when there is a strong relationship among the independent variables.  \n","Ridge regression prevents overfitting.\n","Ridge regression controls the magnitude of polynomial coefficients by introducing a hyperparameter, alpha. \n","\n","\n","To determine alpha, you divide your data into training  and validation data. Starting with a small value for alpha, you train the model, make a prediction using the validation data, then calculate the R-squared and store the values. You repeat the value for a larger value of alpha. You repeat the process for different alpha values, training the model, and making a prediction. You select the value of alpha that maximizes R-squared.\n","\n","The highest alpha value is usually the model with the most underfitting. \n","OR\n","The lowest alpha value is usually the model with the most overfitting."]},{"cell_type":"markdown","id":"bcd69f75-ce26-4a1a-9f69-7ac490c42dd9","metadata":{},"source":[" Let's perform a degree two polynomial transformation on our data. \n"]},{"cell_type":"code","execution_count":null,"id":"91373318-62aa-4c74-ba76-ad1077120c46","metadata":{},"outputs":[],"source":["pr=PolynomialFeatures(degree=2)\n","x_train_pr=pr.fit_transform(x_train[['horsepower', 'curb-weight', 'engine-size', 'highway-mpg','normalized-losses','symboling']])\n","x_test_pr=pr.fit_transform(x_test[['horsepower', 'curb-weight', 'engine-size', 'highway-mpg','normalized-losses','symboling']])"]},{"cell_type":"markdown","id":"504ae769-f228-48f0-8920-a3b52e630da5","metadata":{},"source":["Let's create a Ridge regression object, setting the regularization parameter (alpha) to 1 \n"]},{"cell_type":"code","execution_count":48,"id":"6ad86e2d-51ff-42d2-a704-450ab9dfdb86","metadata":{},"outputs":[],"source":["RigeModel=Ridge(alpha=1)"]},{"cell_type":"markdown","id":"f5c21ecf-321b-4ae8-a2a1-298139aff9af","metadata":{},"source":["Like regular regression, you can fit the model using the method <b>fit</b>.\n"]},{"cell_type":"code","execution_count":null,"id":"37e947f8-16ca-4101-8b62-0443fdff0d6e","metadata":{},"outputs":[],"source":["RigeModel.fit(x_train_pr, y_train)"]},{"cell_type":"markdown","id":"e0732a4e-cbb1-46fc-86e7-7d3f0000b0eb","metadata":{},"source":[" Similarly, you can obtain a prediction: \n"]},{"cell_type":"code","execution_count":50,"id":"3d6b9cc6-889d-4ecb-9e3f-30c9e9deb359","metadata":{},"outputs":[],"source":["yhat = RigeModel.predict(x_test_pr)"]},{"cell_type":"markdown","id":"1b37d949-d67b-4633-966a-627359cdbc45","metadata":{},"source":["Let's compare the first four predicted samples to our test set: \n"]},{"cell_type":"code","execution_count":null,"id":"c26d376f-f7ed-4980-892f-609ef7c568a3","metadata":{},"outputs":[],"source":["print('predicted:', yhat[0:4])\n","print('test set :', y_test[0:4].values)"]},{"cell_type":"markdown","id":"bd009f95-1095-4061-a6f3-8fa46358b2c5","metadata":{},"source":["We select the value of alpha that minimizes the test error. To do so, we can use a for loop. We have also created a progress bar to see how many iterations we have completed so far.\n"]},{"cell_type":"code","execution_count":null,"id":"bcb536fe-8003-4213-a616-6a31c1830ff6","metadata":{},"outputs":[],"source":["from tqdm import tqdm\n","\n","Rsqu_test = []\n","Rsqu_train = []\n","dummy1 = []\n","Alpha = 10 * np.array(range(0,1000))\n","pbar = tqdm(Alpha)\n","\n","for alpha in pbar:\n","    RigeModel = Ridge(alpha=alpha) \n","    RigeModel.fit(x_train_pr, y_train)\n","    test_score, train_score = RigeModel.score(x_test_pr, y_test), RigeModel.score(x_train_pr, y_train)\n","    \n","    pbar.set_postfix({\"Test Score\": test_score, \"Train Score\": train_score})\n","\n","    Rsqu_test.append(test_score)\n","    Rsqu_train.append(train_score)"]},{"cell_type":"markdown","id":"94a88e00-fbea-4a4c-95e2-9ea34af3b7f0","metadata":{},"source":["We can plot out the value of R^2 for different alphas: \n"]},{"cell_type":"code","execution_count":null,"id":"a22e0918-edac-4afd-a9d2-240b84e80a4e","metadata":{},"outputs":[],"source":["width = 12\n","height = 10\n","plt.figure(figsize=(width, height))\n","\n","plt.plot(Alpha,Rsqu_test, label='validation data  ')\n","plt.plot(Alpha,Rsqu_train, 'r', label='training Data ')\n","plt.xlabel('alpha')\n","plt.ylabel('R^2')\n","plt.legend()"]},{"cell_type":"markdown","id":"e520fbe5-df77-48bb-8eab-e6a657aed68f","metadata":{},"source":["**Figure 4**: The blue line represents the R^2 of the validation data, and the red line represents the R^2 of the training data. The x-axis represents the different values of Alpha. \n"]},{"cell_type":"markdown","id":"2df3eb14-185a-4985-b1f8-90637e0699e5","metadata":{},"source":["Here the model is built and tested on the same data, so the training and test data are the same.\n","\n","The red line in Figure 4 represents the R^2 of the training data. As alpha increases the R^2 decreases. Therefore, as alpha increases, the model performs worse on the training data\n","\n","The blue line represents the R^2 on the validation data. As the value for alpha increases, the R^2 increases and converges at a point.\n"]},{"cell_type":"markdown","id":"d14ceea9-c65d-4096-b22d-cd0f938f7207","metadata":{},"source":["<h2 id=\"ref4\"> Grid Search</h2>\n","\n","## method1: Ridge Regression \n"]},{"cell_type":"code","execution_count":null,"id":"eb7e8f06-1a0d-47c2-a6df-4dec4f4a3aaa","metadata":{},"outputs":[],"source":["# The term alpha is a hyperparameter. Sklearn has the class GridSearchCV to make the process of finding the best hyperparameter simpler.\n","# We create a dictionary of parameter values:\n","parameters1= [{'alpha': [0.001,0.1,1, 10, 100, 1000, 10000, 100000, 100000]}]\n","\n","# Create a Ridge regression object:\n","RR=Ridge()\n","\n","# Create a ridge grid search object:\n","Grid1 = GridSearchCV(RR, parameters1,cv=4)\n","\n","# Fit the model:\n","Grid1.fit(x_data[['horsepower', 'curb-weight', 'engine-size', 'highway-mpg']], y_data)\n","\n","# The object finds the best parameter values on the validation data. We can obtain the estimator with the best parameters and assign it to the variable BestRR as follows:\n","BestRR=Grid1.best_estimator_\n","\n","# We now test our model on the test data:\n","BestRR.score(x_test[['horsepower', 'curb-weight', 'engine-size', 'highway-mpg']], y_test)\n","\n","\n","# Perform a grid search for the alpha parameter and the normalization parameter, then find the best values of the parameters:\n","best_alpha = Grid1.best_params_['alpha']\n","best_ridge_model = Ridge(alpha=best_alpha)\n","best_ridge_model.fit(x_data[['horsepower', 'curb-weight', 'engine-size', 'highway-mpg']], y_data)"]},{"cell_type":"markdown","metadata":{},"source":["## method2: Decision Tree "]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# param_grid = {'min_samples_leaf': range(1,50)} #Tuning min_samples_leaf for controlling tree growth.\n","# cv_grid = GridSearchCV(estimator = dtree, param_grid = param_grid, cv = 5, verbose=2, n_jobs=-1)\n","# \"\"\"\n","# grid search model\n","\n","# cv_grid.fit(X, y)\n","# cv_grid.best_estimator_\n","# cv_grid.cv_results_\n","# cv_grid.cv_results_['mean_test_score']\n","# cv_grid.cv_results_['std_test_score']"]},{"cell_type":"markdown","metadata":{},"source":["### **Comparison of LDA and PCA**:\n","\n","| **Feature**                   | **Linear Discriminant Analysis (LDA)** | **Principal Component Analysis (PCA)** |\n","|-------------------------------|----------------------------------------|----------------------------------------|\n","| **Purpose**                    | Supervised dimensionality reduction for classification | Unsupervised dimensionality reduction for capturing variance |\n","| **Input Data**                 | Requires class labels (supervised)     | No need for class labels (unsupervised) |\n","| **Assumptions**                | Assumes normal distribution and equal covariance for each class | Assumes no specific distribution, focuses on variance |\n","| **Goal**                        | Maximizes class separability by finding linear combinations of features | Maximizes the variance of data without considering class labels |\n","| **Dimensionality Reduction**   | Reduces dimensions while preserving class separability | Reduces dimensions by finding directions with the highest variance |\n","| **Use Case**                   | Classification tasks, especially when classes are well-separated | Feature reduction, noise removal, and visualization |\n","| **Output**                     | Projects data into a lower-dimensional space for classification | Projects data into a new feature space based on variance |\n","| **Interpretability**           | The components have direct class-related meanings | Components are based on variance and may not have direct class interpretations |\n","| **Data Requirement**           | Requires labeled data for training | Works with unlabeled data |\n","| **Performance**                | Better performance for classification tasks with well-separated classes | Better for exploratory data analysis and when labels are not available |\n","\n"]},{"cell_type":"markdown","metadata":{},"source":["### **Example for Linear Discriminant Analysis (LDA)**:\n","LDA is used when you have labeled data and want to reduce dimensionality while keeping the class separability intact."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import pandas as pd\n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","\n","\n","X = df_clean.drop(['symboling'], axis=1)  # Features\n","y = df_clean['symboling']  # Target variable: 'symboling' for classification\n","\n","# Split the data into training and test sets\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","\n","# Standardize the features\n","scaler = StandardScaler()\n","X_train_scaled = scaler.fit_transform(X_train)\n","X_test_scaled = scaler.transform(X_test)\n","\n","# Apply Linear Discriminant Analysis (LDA) for dimensionality reduction\n","lda = LinearDiscriminantAnalysis(n_components=2)  # We use 2 components for 2D visualization\n","X_train_lda = lda.fit_transform(X_train_scaled, y_train)\n","X_test_lda = lda.transform(X_test_scaled)\n","\n","# Create a DataFrame for visualization\n","tmp_Df = pd.DataFrame(X_train_lda, columns=['LDA Component 1', 'LDA Component 2'])  # Two components\n","tmp_Df['Class'] = y_train  # Add the target variable (symboling) to the DataFrame\n","\n","# Visualize the LDA components with Seaborn's FacetGrid\n","sns.set(style=\"white\", palette=\"muted\")\n","g = sns.FacetGrid(tmp_Df, hue=\"Class\", height=6)\n","g.map(plt.scatter, 'LDA Component 1', 'LDA Component 2', edgecolor=\"w\", s=100)  # Plot in 2D\n","g.add_legend()\n","\n","# Set plot titles and labels\n","plt.title('LDA: Training Data in 2D', fontsize=16)\n","plt.xlabel('LDA Component 1')\n","plt.ylabel('LDA Component 2')\n","\n","# Show the plot\n","plt.show()\n"]},{"cell_type":"markdown","metadata":{},"source":["### **Example for Principal Component Analysis (PCA)**:\n","\n","PCA is used for dimensionality reduction when you do not have class labels and simply want to reduce the number of features based on variance.\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\n","from sklearn.decomposition import PCA\n","\n","# Standardize the features\n","scaler = StandardScaler()\n","X_train_scaled = scaler.fit_transform(X_train)\n","X_test_scaled = scaler.transform(X_test)\n","\n","# Apply PCA for dimensionality reduction (2 components for 2D visualization)\n","pca = PCA(n_components=2)  # Reduce to 2 components for 2D visualization\n","X_train_pca = pca.fit_transform(X_train_scaled)\n","X_test_pca = pca.transform(X_test_scaled)\n","\n","# Create a DataFrame for visualization\n","tmp_Df = pd.DataFrame(X_train_pca, columns=['PCA Component 1', 'PCA Component 2'])\n","tmp_Df['Class'] = y_train  # Add the target variable (symboling) to the DataFrame\n","\n","# Visualize the PCA components with Seaborn's FacetGrid\n","sns.set(style=\"white\", palette=\"muted\")\n","g = sns.FacetGrid(tmp_Df, hue=\"Class\", height=6)\n","g.map(plt.scatter, 'PCA Component 1', 'PCA Component 2', edgecolor=\"w\", s=100)  # Plot in 2D\n","g.add_legend()\n","\n","# Set plot titles and labels\n","plt.title('PCA: Training Data in 2D', fontsize=16)\n","plt.xlabel('PCA Component 1')\n","plt.ylabel('PCA Component 2')\n","\n","# Show the plot\n","plt.show()\n"]},{"cell_type":"markdown","metadata":{},"source":["### TO DO"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# reg = linear_model.Lars(n_nonzero_coefs=j, fit_path = False, fit_intercept = False, verbose = True)\n","# \"\"\"\n","# lars\n","\n","# reg.fit(Xtrain,ytrain)\n","# beta = reg.coef_.ravel()\n","# \"\"\"\n","\n","# with warnings.catch_warnings(): # done to disable all the convergence warnings from elastic net\n","#     warnings.simplefilter(\"ignore\")\n","#     model = linear_model.ElasticNetCV(cv=5, l1_ratio = alpha, alphas=lambdas, normalize=True).fit(X, y)\n","\n","# \"\"\"\n","# elastic\n","\n","# model.alphas_\n","# model.mse_path_.mean(axis=-1)\n","# \"\"\"\n","\n","# model = LogisticRegression(penalty = 'l1', C = 1/lambda_, solver='liblinear')\n","# model = model.fit(X_train, y_train)\n","# \"\"\"\n","# LogisticRegression\n","# https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html\n","\n","# .fit(X_train, y_train)\n","# .predict(X_test)\n","# .coef_\n","# \"\"\"\n","\n","# dtree = DecisionTreeRegressor()\n","# dtree = DecisionTreeClassifier()\n","# dtree = DecisionTreeClassifier(min_samples_leaf=min_sample_leaf_opt)\n","# dtree=DecisionTreeClassifier(ccp_alpha=0.02040816326530612, criterion='gini')\n","# \"\"\" \n","# create a decisiontreeregressor/classifier \n","# See week 05: \n","# for how to tune the parameter, MinLeaf value, using cross validation.\n","# or to find the tree size through cost complexity pruning of the best estimator\n","\n","# dtree.fit(x, y)\n","# \"\"\"\n","\n","# bagging = BaggingClassifier(DecisionTreeClassifier(), bootstrap=True, oob_score = True)\n","# \"\"\"\n","# bagged trees\n","# \"\"\"\n","\n","# from sklearn.tree import plot_tree\n","# plot_tree(dtree,feature_names = feature_names,filled = True)\n","# \"\"\"\n","# to plot tree\n","\n","# A little description of the information at each plotted node\n","# 1. row: The condition\n","# 2. row: The impurity score of the node\n","# 3. row: The number of observations at this node\n","# 4. row: The number of samples for each class at this node\n","# 5. row: The class by majority voting\n","# \"\"\"\n","\n","# sk.metrics.log_loss\n","# \"\"\"\n","# https://scikit-learn.org/stable/modules/generated/sklearn.metrics.log_loss.html\n","# \"\"\"\n","\n","\n","# clf = RandomForestClassifier(bootstrap=True, oob_score=True, criterion = 'gini',random_state=0)\n","# \"\"\"\n","# \"\"\""]}],"metadata":{"kernelspec":{"display_name":"conda_hadeel","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.10"}},"nbformat":4,"nbformat_minor":4}
